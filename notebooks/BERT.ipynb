{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a7f03e44-6466-4f8d-a651-0733ef29f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set(style='darkgrid', palette='muted', font_scale=1.2)\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2a3a4857-04b8-414e-8074-90a21726a369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a8b0651090>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RANDOM_SEED = 777\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafc8d8-4f29-4d64-b00c-8c5415c3de1e",
   "metadata": {},
   "source": [
    "# Данные - Decorative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b3a6f17b-98f0-4f36-a412-117ae3e56cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>frequency</th>\n",
       "      <th>words_ordered</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>51</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--</td>\n",
       "      <td>4</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>помада -бомба</td>\n",
       "      <td>4</td>\n",
       "      <td>-бомба помада</td>\n",
       "      <td>-бомб помад</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>цвет -бомба</td>\n",
       "      <td>11</td>\n",
       "      <td>-бомба цвет</td>\n",
       "      <td>-бомб цвет</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>оттенок -в</td>\n",
       "      <td>3</td>\n",
       "      <td>-в оттенок</td>\n",
       "      <td>-в оттенок</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567008</th>\n",
       "      <td>ящичке</td>\n",
       "      <td>16</td>\n",
       "      <td>ящичке</td>\n",
       "      <td>ящичк</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567009</th>\n",
       "      <td>ящички</td>\n",
       "      <td>3</td>\n",
       "      <td>ящички</td>\n",
       "      <td>ящичк</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567010</th>\n",
       "      <td>єффект</td>\n",
       "      <td>3</td>\n",
       "      <td>єффект</td>\n",
       "      <td>єффект</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567011</th>\n",
       "      <td>ұнады</td>\n",
       "      <td>6</td>\n",
       "      <td>ұнады</td>\n",
       "      <td>ұнад</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567012</th>\n",
       "      <td>өте</td>\n",
       "      <td>3</td>\n",
       "      <td>өте</td>\n",
       "      <td>өте</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567013 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  item  frequency  words_ordered stemmed_text  count_words\n",
       "0                    -         51              -            -            1\n",
       "1                   --          4             --           --            1\n",
       "2        помада -бомба          4  -бомба помада  -бомб помад            2\n",
       "3          цвет -бомба         11    -бомба цвет   -бомб цвет            2\n",
       "4           оттенок -в          3     -в оттенок   -в оттенок            2\n",
       "...                ...        ...            ...          ...          ...\n",
       "1567008         ящичке         16         ящичке        ящичк            1\n",
       "1567009         ящички          3         ящички        ящичк            1\n",
       "1567010         єффект          3         єффект       єффект            1\n",
       "1567011          ұнады          6          ұнады         ұнад            1\n",
       "1567012            өте          3            өте          өте            1\n",
       "\n",
       "[1567013 rows x 5 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = pd.read_csv('../data/raw/Decorative/Full.csv')\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8bab4451-a9f7-42ba-a83b-81e41e93a8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Stemma</th>\n",
       "      <th>Synonyms</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cлой</td>\n",
       "      <td>топовый слой тож</td>\n",
       "      <td>топовый слой тоже</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mini-size</td>\n",
       "      <td>есть мин</td>\n",
       "      <td>есть мини</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mini-size</td>\n",
       "      <td>есть мин</td>\n",
       "      <td>есть минусы</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mini-size</td>\n",
       "      <td>миниатюр</td>\n",
       "      <td>баночка миниатюрная</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mini-size</td>\n",
       "      <td>миниатюр</td>\n",
       "      <td>довольно миниатюрна</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202301</th>\n",
       "      <td>Яркий макияж</td>\n",
       "      <td>любите яркие цвет</td>\n",
       "      <td>любите яркие цвета</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202302</th>\n",
       "      <td>Яркий макияж</td>\n",
       "      <td>любит яркие оттенк</td>\n",
       "      <td>любит яркие оттенки</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202303</th>\n",
       "      <td>Яркий макияж</td>\n",
       "      <td>теряет свою яркост</td>\n",
       "      <td>теряет свою яркость</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202304</th>\n",
       "      <td>Яркий макияж</td>\n",
       "      <td>такой насыщенный и ярк</td>\n",
       "      <td>такой насыщенный и яркий</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202305</th>\n",
       "      <td>Яркий макияж</td>\n",
       "      <td>ярчайший вечерний макияж</td>\n",
       "      <td>ярчайший вечерний макияжа</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202306 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Topic                    Stemma                   Synonyms  \\\n",
       "0               Cлой          топовый слой тож          топовый слой тоже   \n",
       "1          Mini-size                  есть мин                  есть мини   \n",
       "2          Mini-size                  есть мин                есть минусы   \n",
       "3          Mini-size                  миниатюр        баночка миниатюрная   \n",
       "4          Mini-size                  миниатюр        довольно миниатюрна   \n",
       "...              ...                       ...                        ...   \n",
       "202301  Яркий макияж         любите яркие цвет         любите яркие цвета   \n",
       "202302  Яркий макияж        любит яркие оттенк        любит яркие оттенки   \n",
       "202303  Яркий макияж        теряет свою яркост        теряет свою яркость   \n",
       "202304  Яркий макияж    такой насыщенный и ярк   такой насыщенный и яркий   \n",
       "202305  Яркий макияж  ярчайший вечерний макияж  ярчайший вечерний макияжа   \n",
       "\n",
       "        Result  \n",
       "0            1  \n",
       "1            1  \n",
       "2            0  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "202301       0  \n",
       "202302       0  \n",
       "202303       0  \n",
       "202304       0  \n",
       "202305       1  \n",
       "\n",
       "[202306 rows x 4 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms = pd.read_csv('../data/raw/Decorative/Synonyms.csv')\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "11888596-fa3e-461e-b90d-693b344afaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>frequency</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>веко -выкинула</td>\n",
       "      <td>30</td>\n",
       "      <td>Веки</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>гель -лака</td>\n",
       "      <td>38</td>\n",
       "      <td>Продукт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>фирма -меня</td>\n",
       "      <td>15</td>\n",
       "      <td>Производитель</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>густой -получилось</td>\n",
       "      <td>78</td>\n",
       "      <td>Консистенция</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>бежевый ..</td>\n",
       "      <td>192</td>\n",
       "      <td>Цвет/оттенок/тон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171100</th>\n",
       "      <td>ярко-салатовый</td>\n",
       "      <td>15</td>\n",
       "      <td>Цвет/оттенок/тон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171101</th>\n",
       "      <td>ярко-синий</td>\n",
       "      <td>54</td>\n",
       "      <td>Цвет/оттенок/тон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171102</th>\n",
       "      <td>ярко-фиолетовый</td>\n",
       "      <td>13</td>\n",
       "      <td>Цвет/оттенок/тон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171103</th>\n",
       "      <td>ярко-черная</td>\n",
       "      <td>18</td>\n",
       "      <td>Цвет/оттенок/тон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171104</th>\n",
       "      <td>яркорозовый</td>\n",
       "      <td>10</td>\n",
       "      <td>Цвет/оттенок/тон</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171105 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review  frequency         sentiment\n",
       "0           веко -выкинула         30              Веки\n",
       "1               гель -лака         38           Продукт\n",
       "2              фирма -меня         15     Производитель\n",
       "3       густой -получилось         78      Консистенция\n",
       "4               бежевый ..        192  Цвет/оттенок/тон\n",
       "...                    ...        ...               ...\n",
       "171100      ярко-салатовый         15  Цвет/оттенок/тон\n",
       "171101          ярко-синий         54  Цвет/оттенок/тон\n",
       "171102     ярко-фиолетовый         13  Цвет/оттенок/тон\n",
       "171103         ярко-черная         18  Цвет/оттенок/тон\n",
       "171104         яркорозовый         10  Цвет/оттенок/тон\n",
       "\n",
       "[171105 rows x 3 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = full.merge(synonyms, left_on='item', right_on='Synonyms')\n",
    "df = df.loc[df['Result'] == 1, ['item', 'frequency', 'Topic']].reset_index(drop=True)\n",
    "df.rename(columns={'item': 'review', 'Topic': 'sentiment'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "545724d9-dbfc-4a98-9df8-1a98e01ace90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 27, 227, 229, 113, 334,  15, 279,  23,  39,  44,  51,  57, 135,\n",
       "       109, 169, 128, 204, 188, 168, 268, 181, 193, 228, 237, 282, 242,\n",
       "       251, 267, 305, 341, 346, 147, 337,  34,  18,  89, 139,  19, 102,\n",
       "       294, 350, 187, 324, 107, 127, 215, 106, 165, 254, 244, 157,  68,\n",
       "       137, 140, 122, 287,  10,  53, 319,  76, 173, 246, 329, 166, 199,\n",
       "       116, 338, 325,  93,  43, 272,  90,  92, 271, 288, 269, 344, 349,\n",
       "        38,  32, 232, 170, 141, 347, 119, 196, 261, 308, 262,  95, 286,\n",
       "        77, 180, 112, 197, 125, 138, 131, 146, 236, 239, 283, 222, 213,\n",
       "       245, 302, 240,  83, 317,  13, 277, 105,  17,  79,  61, 295, 296,\n",
       "       320, 241, 275, 117, 276, 100, 121, 171, 326, 316,  33, 322,   3,\n",
       "        52,  81,  66, 339,  47,  58,  70, 203, 153, 164, 192, 292,  94,\n",
       "       219,   4, 273,   5, 278, 145,  73,  98,   6,   7, 335, 179, 309,\n",
       "       115, 238, 206, 318,  28,  35,   8,  14, 291,   9, 285,  96, 103,\n",
       "       207, 208, 218, 118, 220, 177, 243, 255, 152, 163,  65, 205, 270,\n",
       "       104, 126, 143, 253, 328, 280,  46,   1,  11,  56, 108,  88,  12,\n",
       "       301,  16, 233, 191, 111,  63,  67,  85, 313,  64, 235, 162, 260,\n",
       "       259,  78, 330, 178, 250, 190,  71, 123, 327, 290, 231,  75, 252,\n",
       "       212, 274,  82,  31,  36,  25, 174, 202, 321,  30,  45,   2, 195,\n",
       "        49, 189, 249,  20, 185, 307,  22,  86, 158, 120, 133, 312, 348,\n",
       "        21, 257,  59, 159, 201, 225, 172, 194, 214, 300, 333, 210, 258,\n",
       "       150,  72, 299,  41, 144, 223, 314,  97, 161, 198,  26, 142, 226,\n",
       "       311, 224,  37, 124, 134,  69, 200, 217,  87, 298, 266, 167, 304,\n",
       "       149,  74, 336, 209, 101, 186, 175,  48, 247, 289, 323, 332, 340,\n",
       "       315, 264, 263,  29,  91, 351, 303,  42, 234, 310,  24, 156, 136,\n",
       "        40,  50, 183,  84, 216, 284,  54,  80,  60, 221,  55, 114, 281,\n",
       "       331, 182, 265, 230, 342,  62, 343,  99, 151, 256, 297, 154, 132,\n",
       "       211, 110, 176, 184, 130, 148, 248, 155, 160, 129, 293,   0, 345,\n",
       "       306])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encode = LabelEncoder()\n",
    "\n",
    "df['sentiment'] = encode.fit_transform(df['sentiment'])\n",
    "class_names = df['sentiment'].unique()\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be985fb-85c0-4dc9-8bfa-dda8b9110bae",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "22306190-3cdd-4abb-8c05-47c31ad7c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'cointegrated/rubert-tiny'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a0ce2ee1-8f5c-444e-90a5-73fe3641fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 254\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "          'review_text': review,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "310b24df-9540-4355-bf2e-08c23865bb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45053, 3), (1351, 3), (66229, 3))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.6, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.98, random_state=RANDOM_SEED)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2c87e04f-29a8-452f-9197-e1fa162dd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = ReviewDataset(\n",
    "        reviews=df.review.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len)\n",
    "\n",
    "    return DataLoader(ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6d7b98e1-3937-4049-bd5e-f0515f673c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5033-6972-48b5-ba28-2218cb3973d5",
   "metadata": {},
   "source": [
    "# MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "acac1adc-dab0-48ae-b9c0-4f0da199ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('cointegrated/rubert-tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a97d1b4e-fad4-49ec-b451-5ea03b031a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.drop(outputs[\"pooler_output\"])\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec0dd56b-93b1-4c00-9bab-b259d58b0779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29564, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (out): Linear(in_features=312, out_features=252, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e612018-c303-49ee-a0df-42bfae5e073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "  \n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ec29450a-3be2-4072-b4ca-393b14016d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f2aec2a3-d6a1-4d1e-976e-1c6819e8140e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd40419c4a264c809b2dad32b4cf3c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a4d6993ba34318acb2ae7f8779837b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.9262768057682975 accuracy 0.8006348078929262\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653da82204b3479b9f9ec1169b9255cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.9509508093429166 accuracy 0.7831236121391562\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06581f031bd24c54a59239e86b2ff39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.8396292509913276 accuracy 0.8132643775109316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124c438b328b44c6a0537944be043766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.8906586087027262 accuracy 0.7920059215396003\n",
      "\n",
      "CPU times: total: 41.5 s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, len(df_train))\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))\n",
    "\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state_2.bin')\n",
    "        best_accuracy = val_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e3eae76c-c445-408e-a3a0-1273ddb380fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6264bf82a7d4f219a209e1e7d12f179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7958900179679597"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4e63ce9d-202a-402c-a165-425c5c9471bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "\n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1f4c94a5-b542-43ca-aa70-3b4ed98216de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8d22b51e-be15-47ba-93c4-73821d5d5113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          15       0.81      0.82      0.81       340\n",
      "         159       0.89      0.94      0.92      1038\n",
      "          74       0.87      0.89      0.88      1087\n",
      "           7       0.90      0.95      0.92       742\n",
      "         200       0.93      0.96      0.95       528\n",
      "          14       0.84      0.75      0.79       717\n",
      "          24       0.57      0.89      0.69        79\n",
      "          27       0.86      0.97      0.91       118\n",
      "          32       0.93      0.91      0.92       494\n",
      "          36       0.76      0.99      0.86        92\n",
      "          88       0.79      0.69      0.74       248\n",
      "          71       0.75      0.88      0.81        85\n",
      "         111       0.93      0.98      0.95       415\n",
      "          86       0.89      0.90      0.90       268\n",
      "         138       0.83      0.87      0.85       294\n",
      "         124       0.54      0.10      0.17       127\n",
      "         110       0.89      0.74      0.81        77\n",
      "         191       0.88      0.94      0.91      1471\n",
      "         120       0.82      0.74      0.78       881\n",
      "         129       0.85      0.95      0.90       265\n",
      "         158       0.85      0.93      0.88       322\n",
      "         166       0.77      0.83      0.80       160\n",
      "         202       0.80      0.87      0.83      3254\n",
      "         171       0.60      0.94      0.73       277\n",
      "         179       0.72      0.95      0.82       293\n",
      "         190       0.90      0.96      0.93        54\n",
      "         219       0.87      0.94      0.91       553\n",
      "         246       0.71      0.92      0.80       131\n",
      "         248       0.76      0.94      0.84       103\n",
      "          98       0.87      0.94      0.91       416\n",
      "         242       0.71      0.78      0.74       576\n",
      "          19       1.00      0.55      0.71        29\n",
      "           9       0.86      0.80      0.83       967\n",
      "          57       0.96      0.98      0.97      1513\n",
      "          92       0.94      0.98      0.96       465\n",
      "          10       0.89      0.88      0.88       626\n",
      "          66       0.90      0.82      0.86      1469\n",
      "         209       0.83      0.89      0.86       604\n",
      "         251       0.78      0.71      0.74      1137\n",
      "         123       0.76      0.84      0.80       983\n",
      "         232       0.83      0.95      0.88        97\n",
      "          69       0.91      1.00      0.95       110\n",
      "          85       0.95      0.94      0.94       173\n",
      "         148       0.84      0.91      0.87       760\n",
      "         107       0.00      0.00      0.00        46\n",
      "         182       0.95      0.98      0.96       257\n",
      "         173       0.57      0.70      0.63      2280\n",
      "          44       0.70      0.62      0.66       376\n",
      "          90       0.84      0.89      0.86       441\n",
      "          93       0.87      0.87      0.87       215\n",
      "          81       0.77      0.91      0.84        92\n",
      "         206       0.88      0.08      0.14        93\n",
      "           3       0.64      0.93      0.76       159\n",
      "          34       0.86      0.94      0.90      1023\n",
      "         229       0.82      0.93      0.87       276\n",
      "          51       0.90      0.84      0.87       164\n",
      "         115       0.78      0.80      0.79       892\n",
      "         175       0.81      0.85      0.83       269\n",
      "         236       0.00      0.00      0.00        56\n",
      "         108       0.91      0.98      0.95       395\n",
      "         134       0.80      0.96      0.87       147\n",
      "          76       0.70      0.92      0.79       220\n",
      "         243       0.87      0.93      0.90      2231\n",
      "         233       0.00      0.00      0.00        42\n",
      "          60       0.83      0.40      0.54        96\n",
      "          26       0.77      0.92      0.84        77\n",
      "         195       0.92      0.60      0.73        40\n",
      "          58       0.71      0.92      0.80        88\n",
      "          59       0.50      0.09      0.15        22\n",
      "         194       0.76      0.89      0.82        94\n",
      "         192       0.85      0.95      0.90      1012\n",
      "         247       0.81      0.78      0.79        27\n",
      "         250       0.83      0.66      0.74        38\n",
      "          23       0.00      0.00      0.00        30\n",
      "         161       0.73      0.67      0.70       349\n",
      "         112       0.50      0.86      0.63        90\n",
      "          94       0.72      0.93      0.81       162\n",
      "         249       0.97      0.88      0.92        41\n",
      "         131       0.60      0.31      0.41       107\n",
      "         187       0.59      0.71      0.64       118\n",
      "         221       0.62      0.78      0.69       606\n",
      "         188       0.78      0.86      0.81       359\n",
      "          62       0.67      0.65      0.66       455\n",
      "         205       0.64      0.81      0.71       505\n",
      "          52       0.80      0.80      0.80       320\n",
      "         119       0.86      0.90      0.88       344\n",
      "          73       0.83      0.38      0.52        66\n",
      "         132       0.81      0.29      0.43       103\n",
      "          83       0.83      0.96      0.89       196\n",
      "          91       0.90      0.93      0.92       390\n",
      "          87       0.79      0.81      0.80       382\n",
      "          97       0.89      0.88      0.89       190\n",
      "         165       0.75      0.72      0.73        53\n",
      "         168       0.86      0.69      0.76       254\n",
      "         203       0.90      0.32      0.48        59\n",
      "         152       0.00      0.00      0.00        34\n",
      "         146       0.66      0.67      0.66       139\n",
      "         174       0.77      0.92      0.84       787\n",
      "         216       0.85      0.94      0.89       356\n",
      "         169       1.00      0.03      0.06        63\n",
      "         228       0.67      0.06      0.10       143\n",
      "           5       1.00      0.03      0.05        71\n",
      "         199       0.94      0.82      0.88       108\n",
      "          68       0.71      0.82      0.76       325\n",
      "          53       0.90      0.17      0.29       105\n",
      "          38       0.74      0.72      0.73       692\n",
      "         210       0.58      0.47      0.52       188\n",
      "         211       0.58      0.17      0.27        40\n",
      "         230       0.84      0.96      0.90       323\n",
      "         170       0.88      0.09      0.17       158\n",
      "         197       0.71      0.80      0.75       820\n",
      "          77       0.88      0.88      0.88        40\n",
      "         198       0.78      0.88      0.82      1413\n",
      "          65       0.74      0.96      0.84        56\n",
      "          80       0.82      0.97      0.89       188\n",
      "         113       0.77      0.54      0.63       153\n",
      "         234       0.70      0.58      0.64       779\n",
      "         227       0.85      0.31      0.45        55\n",
      "          18       0.38      0.35      0.37       113\n",
      "         231       0.73      0.86      0.79       214\n",
      "           1       0.76      0.75      0.76       102\n",
      "          33       0.00      0.00      0.00        31\n",
      "          54       0.78      0.92      0.85       378\n",
      "          42       0.46      0.49      0.48       219\n",
      "         244       0.00      0.00      0.00        26\n",
      "          30       0.96      0.51      0.67        49\n",
      "          37       0.85      0.87      0.86        63\n",
      "          46       0.55      0.56      0.56       698\n",
      "         137       0.76      0.77      0.77       657\n",
      "         101       0.87      0.73      0.79        92\n",
      "         106       0.91      1.00      0.95       127\n",
      "         128       1.00      0.03      0.07        29\n",
      "         208       0.63      0.84      0.72       113\n",
      "          61       0.81      0.85      0.83        82\n",
      "         150       0.00      0.00      0.00        53\n",
      "           2       0.82      0.94      0.88       172\n",
      "          96       1.00      0.02      0.04        55\n",
      "          48       1.00      0.60      0.75        45\n",
      "          64       0.91      0.85      0.88        62\n",
      "         240       0.00      0.00      0.00        36\n",
      "         118       0.77      0.49      0.60        35\n",
      "         222       0.50      0.12      0.19       130\n",
      "          75       0.00      0.00      0.00        28\n",
      "         167       0.00      0.00      0.00        52\n",
      "         140       0.78      0.96      0.86       123\n",
      "          20       0.64      0.89      0.74       142\n",
      "           6       0.88      0.50      0.64        44\n",
      "         207       0.89      0.91      0.90       643\n",
      "         204       0.73      0.71      0.72       195\n",
      "          67       0.80      0.91      0.85       321\n",
      "         141       0.65      0.84      0.73       640\n",
      "         142       0.64      0.20      0.30       262\n",
      "         149       0.96      0.17      0.29       139\n",
      "          78       0.84      0.94      0.89        63\n",
      "         151       0.56      0.90      0.69       116\n",
      "         117       0.78      0.72      0.75        39\n",
      "         172       0.86      0.65      0.74       269\n",
      "         183       1.00      0.39      0.57        33\n",
      "         100       0.63      0.73      0.68       181\n",
      "         105       0.67      0.85      0.75       227\n",
      "          41       0.75      0.14      0.24        21\n",
      "         139       0.95      0.82      0.88        67\n",
      "         193       0.88      0.08      0.14        91\n",
      "          84       1.00      0.12      0.21        34\n",
      "          95       0.00      0.00      0.00        31\n",
      "         181       0.95      0.25      0.40       207\n",
      "         235       0.00      0.00      0.00        63\n",
      "         201       0.00      0.00      0.00        32\n",
      "          29       0.79      0.63      0.70       118\n",
      "           0       0.50      0.04      0.07        28\n",
      "           4       0.83      0.98      0.90        87\n",
      "          35       0.96      0.41      0.57        56\n",
      "          70       0.76      0.99      0.86       133\n",
      "           8       0.82      0.93      0.87        83\n",
      "         162       0.95      0.93      0.94       107\n",
      "         127       0.79      0.83      0.81       120\n",
      "          72       0.66      0.84      0.74       138\n",
      "          39       0.57      0.39      0.47       127\n",
      "          43       0.00      0.00      0.00        53\n",
      "          56       0.61      0.79      0.69       104\n",
      "         224       0.72      0.78      0.75        40\n",
      "          40       0.00      0.00      0.00        86\n",
      "         164       0.84      0.62      0.72       104\n",
      "         104       1.00      0.24      0.38        38\n",
      "         186       0.76      0.83      0.80       101\n",
      "         185       0.41      0.29      0.34       104\n",
      "         237       0.67      0.06      0.11        34\n",
      "         178       0.63      0.20      0.30        61\n",
      "         126       0.93      0.54      0.69        68\n",
      "          47       0.78      0.79      0.79        92\n",
      "          50       0.88      0.86      0.87        49\n",
      "         180       0.95      0.91      0.93        23\n",
      "         145       0.75      0.29      0.42        52\n",
      "         196       0.95      0.70      0.81        83\n",
      "          55       0.38      0.07      0.12        69\n",
      "          17       0.00      0.00      0.00        32\n",
      "          21       0.85      0.40      0.54        43\n",
      "         116       0.00      0.00      0.00        22\n",
      "         136       0.93      0.95      0.94       102\n",
      "          16       0.00      0.00      0.00        34\n",
      "          28       0.00      0.00      0.00        41\n",
      "         130       0.75      0.84      0.79       113\n",
      "          31       0.00      0.00      0.00        32\n",
      "         125       0.40      0.83      0.54        54\n",
      "         177       0.55      0.30      0.38        61\n",
      "          11       1.00      0.32      0.48        47\n",
      "         122       1.00      0.22      0.36        37\n",
      "         220       0.71      0.81      0.76       135\n",
      "          13       0.00      0.00      0.00        72\n",
      "          79       0.00      0.00      0.00        49\n",
      "          12       0.87      0.92      0.89        84\n",
      "         184       0.63      0.60      0.62        43\n",
      "         102       0.88      0.69      0.77        42\n",
      "         135       0.89      0.97      0.93        88\n",
      "         155       0.89      0.61      0.72        28\n",
      "         114       0.00      0.00      0.00        19\n",
      "         147       0.75      0.13      0.23        68\n",
      "         215       1.00      0.03      0.06        31\n",
      "         238       0.00      0.00      0.00        31\n",
      "         144       0.90      0.90      0.90       143\n",
      "          99       1.00      0.48      0.65        21\n",
      "         214       0.00      0.00      0.00        45\n",
      "          25       0.79      0.82      0.80        92\n",
      "         153       0.00      0.00      0.00        30\n",
      "         225       0.57      0.99      0.72        79\n",
      "          63       1.00      0.17      0.29        53\n",
      "         103       0.73      0.83      0.78        70\n",
      "         133       0.00      0.00      0.00        26\n",
      "         156       1.00      0.54      0.70        28\n",
      "         223       0.46      0.88      0.61        82\n",
      "         154       0.84      0.84      0.84       110\n",
      "          22       1.00      0.07      0.13        42\n",
      "          82       0.90      0.33      0.49        27\n",
      "          45       0.00      0.00      0.00        72\n",
      "         213       0.00      0.00      0.00        49\n",
      "         189       0.76      0.74      0.75        39\n",
      "         109       0.00      0.00      0.00        80\n",
      "         218       0.80      0.97      0.88       109\n",
      "          49       1.00      0.33      0.50        33\n",
      "         241       0.00      0.00      0.00        23\n",
      "         143       0.48      0.80      0.60        55\n",
      "         176       0.83      0.10      0.18        49\n",
      "         245       0.90      0.43      0.58        21\n",
      "         226       0.00      0.00      0.00        33\n",
      "         217       0.90      0.15      0.25        61\n",
      "         163       0.93      0.85      0.89        46\n",
      "          89       0.80      0.29      0.43        68\n",
      "         121       0.00      0.00      0.00        27\n",
      "         160       0.00      0.00      0.00        25\n",
      "         212       0.00      0.00      0.00        33\n",
      "         157       0.86      0.87      0.86      3359\n",
      "         239       0.85      0.93      0.89      2268\n",
      "\n",
      "    accuracy                           0.80     66229\n",
      "   macro avg       0.68      0.59      0.59     66229\n",
      "weighted avg       0.78      0.80      0.78     66229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, labels=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8a71bb57-e54b-4752-b90a-a3474c71c2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_names</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>183</td>\n",
       "      <td>0.930028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>195</td>\n",
       "      <td>0.011675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>241</td>\n",
       "      <td>0.005407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>70</td>\n",
       "      <td>0.002971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>245</td>\n",
       "      <td>0.002670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>219</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>181</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>88</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>194</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_names    values\n",
       "157          183  0.930028\n",
       "66           195  0.011675\n",
       "239          241  0.005407\n",
       "172           70  0.002971\n",
       "242          245  0.002670\n",
       "..           ...       ...\n",
       "26           219  0.000004\n",
       "165          181  0.000004\n",
       "10            88  0.000003\n",
       "65            26  0.000003\n",
       "69           194  0.000003\n",
       "\n",
       "[252 rows x 2 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 10\n",
    "\n",
    "review_text = y_review_texts[idx]\n",
    "true_sentiment = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "  'class_names': class_names,\n",
    "  'values': y_pred_probs[idx]\n",
    "})\n",
    "\n",
    "pred_df.sort_values('values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5a9a6cb3-8296-4edf-8c65-c1cbf373a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "pred = []\n",
    "for idx in range(len(y_review_texts)):\n",
    "\n",
    "    review_text = y_review_texts[idx]\n",
    "    true_sentiment = y_test[idx]\n",
    "    pred_df = pd.DataFrame({\n",
    "      'class_names': class_names,\n",
    "      'values': y_pred_probs[idx]\n",
    "    })\n",
    "    \n",
    "    if any(pred_df.sort_values('values', ascending=False)['values'].values > 0.9):\n",
    "        count += 1\n",
    "    pred.append(pred_df.sort_values('values', ascending=False)['values'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8c7f6a82-9fc2-445e-ab83-ed848330df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "daf558e2-a870-4715-bfa5-b42c1b08ac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7371488891887589"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(np.array(y_test).reshape(-1, 1), pred, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b098c8a-f18a-4aff-b87c-d0ad60c9d645",
   "metadata": {},
   "source": [
    "# Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "777171e2-84f8-44db-a9b4-5a64534b9f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>frequency</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168695</th>\n",
       "      <td>товара</td>\n",
       "      <td>1500335</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170816</th>\n",
       "      <td>цена</td>\n",
       "      <td>152956</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170580</th>\n",
       "      <td>цвета</td>\n",
       "      <td>149952</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155391</th>\n",
       "      <td>помада</td>\n",
       "      <td>148516</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166624</th>\n",
       "      <td>сохнет</td>\n",
       "      <td>103716</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21743</th>\n",
       "      <td>будет стойко</td>\n",
       "      <td>10</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157741</th>\n",
       "      <td>появляется трещина</td>\n",
       "      <td>10</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67682</th>\n",
       "      <td>достаточно смуглой</td>\n",
       "      <td>10</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86901</th>\n",
       "      <td>лица из-за</td>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171104</th>\n",
       "      <td>яркорозовый</td>\n",
       "      <td>10</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112633 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review  frequency  sentiment\n",
       "168695              товара    1500335        157\n",
       "170816                цена     152956        243\n",
       "170580               цвета     149952        239\n",
       "155391              помада     148516        157\n",
       "166624              сохнет     103716         34\n",
       "...                    ...        ...        ...\n",
       "21743         будет стойко         10        202\n",
       "157741  появляется трещина         10        173\n",
       "67682   достаточно смуглой         10        210\n",
       "86901           лица из-за         10         87\n",
       "171104         яркорозовый         10        239\n",
       "\n",
       "[112633 rows x 3 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values('frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc9627-d2f3-47be-aade-72732a9deb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
